{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00839d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a7c82-0fc8-431d-b06a-95d16ed48c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c4cf22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db42d4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x25452805290>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d41cdac",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23a65fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT = 768\n",
    "BATCH = 32\n",
    "\n",
    "VALID_PROP = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb2de18",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2362ba5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at ai-forever/rugpt3small_based_on_gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"ai-forever/rugpt3small_based_on_gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "model.score = torch.nn.Linear(768, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a56f7d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50264, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834e2f3d",
   "metadata": {},
   "source": [
    "Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "185d6200",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d862bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.score.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.transformer.ln_f.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.transformer.h[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.transformer.h[-2].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.transformer.h[-3].parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0f6ef70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Параметры с градиентами:\n",
      "✓ transformer.h.9.ln_1.weight\n",
      "✓ transformer.h.9.ln_1.bias\n",
      "✓ transformer.h.9.attn.c_attn.weight\n",
      "✓ transformer.h.9.attn.c_attn.bias\n",
      "✓ transformer.h.9.attn.c_proj.weight\n",
      "✓ transformer.h.9.attn.c_proj.bias\n",
      "✓ transformer.h.9.ln_2.weight\n",
      "✓ transformer.h.9.ln_2.bias\n",
      "✓ transformer.h.9.mlp.c_fc.weight\n",
      "✓ transformer.h.9.mlp.c_fc.bias\n",
      "✓ transformer.h.9.mlp.c_proj.weight\n",
      "✓ transformer.h.9.mlp.c_proj.bias\n",
      "✓ transformer.h.10.ln_1.weight\n",
      "✓ transformer.h.10.ln_1.bias\n",
      "✓ transformer.h.10.attn.c_attn.weight\n",
      "✓ transformer.h.10.attn.c_attn.bias\n",
      "✓ transformer.h.10.attn.c_proj.weight\n",
      "✓ transformer.h.10.attn.c_proj.bias\n",
      "✓ transformer.h.10.ln_2.weight\n",
      "✓ transformer.h.10.ln_2.bias\n",
      "✓ transformer.h.10.mlp.c_fc.weight\n",
      "✓ transformer.h.10.mlp.c_fc.bias\n",
      "✓ transformer.h.10.mlp.c_proj.weight\n",
      "✓ transformer.h.10.mlp.c_proj.bias\n",
      "✓ transformer.h.11.ln_1.weight\n",
      "✓ transformer.h.11.ln_1.bias\n",
      "✓ transformer.h.11.attn.c_attn.weight\n",
      "✓ transformer.h.11.attn.c_attn.bias\n",
      "✓ transformer.h.11.attn.c_proj.weight\n",
      "✓ transformer.h.11.attn.c_proj.bias\n",
      "✓ transformer.h.11.ln_2.weight\n",
      "✓ transformer.h.11.ln_2.bias\n",
      "✓ transformer.h.11.mlp.c_fc.weight\n",
      "✓ transformer.h.11.mlp.c_fc.bias\n",
      "✓ transformer.h.11.mlp.c_proj.weight\n",
      "✓ transformer.h.11.mlp.c_proj.bias\n",
      "✓ transformer.ln_f.weight\n",
      "✓ transformer.ln_f.bias\n",
      "✓ score.weight\n",
      "✓ score.bias\n",
      "\n",
      "Обучаемые параметры: 21267459/125233923 (17.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Параметры с градиентами:\")\n",
    "total_params = 0\n",
    "trainable_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    total_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "        print(f\"✓ {name}\")\n",
    "\n",
    "print(f\"\\nОбучаемые параметры: {trainable_params}/{total_params} ({trainable_params/total_params*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c631c903",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6ee91d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from analysis.precomposed.TorchDataset import TorchDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a516a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./analysis/data/dataset_onehot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1db8cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = data.sample(n=int(data.shape[0]*VALID_PROP))\n",
    "data_train = data.drop(index=data_val.index)\n",
    "\n",
    "data_train.reset_index(inplace=True, drop=True)\n",
    "data_val.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00e512cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's go!\n",
      "Let's go!\n"
     ]
    }
   ],
   "source": [
    "ds_train = TorchDataset(data= data_train, tokenizer = tokenizer, context_l = CONTEXT)\n",
    "ds_valid = TorchDataset(data= data_val, tokenizer = tokenizer, context_l = CONTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d77c757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size= BATCH, shuffle= True)\n",
    "dl_valid = DataLoader(ds_valid, batch_size= BATCH, shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d25f1",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7337e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eda5b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29d7e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "TOTAL_BATCHES = len(dl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e218688",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = opt.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "scheduler = opt.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "best_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe39a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|████████████████▌                                                                                                                                                     | 2/20 [2:33:45<23:03:50, 4612.79s/it, train_loss=0.1224, val_loss=0.1509, lr=1.00e-04]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "save() got an unexpected keyword argument 'weights_only'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (avg_val_loss < best_val_loss \u001b[38;5;129;01mand\u001b[39;00m avg_val_loss < \u001b[32m0.1605\u001b[39m):\n\u001b[32m     39\u001b[39m     best_val_loss = avg_val_loss\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./analysis/neural_training/rugpt3s_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbest_val_loss\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaved\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: save() got an unexpected keyword argument 'weights_only'"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(range(EPOCHS), desc=\"Epoch\")\n",
    "for epoch in pbar:\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for batch_idx, (data, targets) in enumerate(dl_train):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        if hasattr(outputs, 'logits'):\n",
    "            loss = criterion(outputs.logits, targets)\n",
    "        else:\n",
    "            loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    valid_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, targets) in enumerate(dl_valid):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            if hasattr(outputs, 'logits'):\n",
    "                v_loss = criterion(outputs.logits, targets)\n",
    "            else:\n",
    "                v_loss = criterion(outputs, targets)\n",
    "            valid_losses.append(v_loss.item())\n",
    "\n",
    "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "    avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
    "    scheduler.step()\n",
    "    pbar.set_postfix({\n",
    "        'train_loss': f'{avg_train_loss:.4f}',\n",
    "        'val_loss': f'{avg_val_loss:.4f}',\n",
    "        'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "    })\n",
    "\n",
    "    if (avg_val_loss < best_val_loss and avg_val_loss < 0.1605):\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model, f'./analysis/neural_training/rugpt3s_{int(best_val_loss*10**6)}.pth')\n",
    "        print(\"Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cebbc7",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "575ca92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = torch.load(\"./analysis/neural_training/rugpt3s_160560.pth\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1e82c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def calculate_class_accuracy(model, dataloader, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    _batch = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            _batch+=1\n",
    "            print(f\"[PREDICTIONS] BATCH: {_batch}/{len(dataloader)}\")\n",
    "            if isinstance(batch, (list, tuple)):\n",
    "                inputs = batch[0]\n",
    "                targets = batch[1]\n",
    "            else:\n",
    "                inputs = batch\n",
    "                targets = batch['labels']\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if isinstance(outputs, (tuple, list)):\n",
    "                logits = outputs[0]\n",
    "            else:\n",
    "                logits = outputs\n",
    "            \n",
    "            predictions = torch.argmax(logits.logits, dim=1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(torch.argmax(targets, dim=1).cpu().numpy())\n",
    "    \n",
    "    predictions = np.array(all_predictions)\n",
    "    targets = np.array(all_targets)\n",
    "    \n",
    "    class_accuracies = {}\n",
    "    \n",
    "    for class_idx, class_name in enumerate(['SPAM', 'TOXIC', 'OK']):\n",
    "        class_mask = targets == class_idx\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_accuracy = accuracy_score(\n",
    "                targets[class_mask], \n",
    "                predictions[class_mask]\n",
    "            )\n",
    "            class_accuracies[class_name] = class_accuracy\n",
    "        else:\n",
    "            class_accuracies[class_name] = 0.0\n",
    "    \n",
    "    total_accuracy = accuracy_score(targets, predictions)\n",
    "    class_accuracies['TOTAL'] = total_accuracy\n",
    "    \n",
    "    return class_accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bf0d9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PREDICTIONS] BATCH: 1/178\n",
      "[PREDICTIONS] BATCH: 2/178\n",
      "[PREDICTIONS] BATCH: 3/178\n",
      "[PREDICTIONS] BATCH: 4/178\n",
      "[PREDICTIONS] BATCH: 5/178\n",
      "[PREDICTIONS] BATCH: 6/178\n",
      "[PREDICTIONS] BATCH: 7/178\n",
      "[PREDICTIONS] BATCH: 8/178\n",
      "[PREDICTIONS] BATCH: 9/178\n",
      "[PREDICTIONS] BATCH: 10/178\n",
      "[PREDICTIONS] BATCH: 11/178\n",
      "[PREDICTIONS] BATCH: 12/178\n",
      "[PREDICTIONS] BATCH: 13/178\n",
      "[PREDICTIONS] BATCH: 14/178\n",
      "[PREDICTIONS] BATCH: 15/178\n",
      "[PREDICTIONS] BATCH: 16/178\n",
      "[PREDICTIONS] BATCH: 17/178\n",
      "[PREDICTIONS] BATCH: 18/178\n",
      "[PREDICTIONS] BATCH: 19/178\n",
      "[PREDICTIONS] BATCH: 20/178\n",
      "[PREDICTIONS] BATCH: 21/178\n",
      "[PREDICTIONS] BATCH: 22/178\n",
      "[PREDICTIONS] BATCH: 23/178\n",
      "[PREDICTIONS] BATCH: 24/178\n",
      "[PREDICTIONS] BATCH: 25/178\n",
      "[PREDICTIONS] BATCH: 26/178\n",
      "[PREDICTIONS] BATCH: 27/178\n",
      "[PREDICTIONS] BATCH: 28/178\n",
      "[PREDICTIONS] BATCH: 29/178\n",
      "[PREDICTIONS] BATCH: 30/178\n",
      "[PREDICTIONS] BATCH: 31/178\n",
      "[PREDICTIONS] BATCH: 32/178\n",
      "[PREDICTIONS] BATCH: 33/178\n",
      "[PREDICTIONS] BATCH: 34/178\n",
      "[PREDICTIONS] BATCH: 35/178\n",
      "[PREDICTIONS] BATCH: 36/178\n",
      "[PREDICTIONS] BATCH: 37/178\n",
      "[PREDICTIONS] BATCH: 38/178\n",
      "[PREDICTIONS] BATCH: 39/178\n",
      "[PREDICTIONS] BATCH: 40/178\n",
      "[PREDICTIONS] BATCH: 41/178\n",
      "[PREDICTIONS] BATCH: 42/178\n",
      "[PREDICTIONS] BATCH: 43/178\n",
      "[PREDICTIONS] BATCH: 44/178\n",
      "[PREDICTIONS] BATCH: 45/178\n",
      "[PREDICTIONS] BATCH: 46/178\n",
      "[PREDICTIONS] BATCH: 47/178\n",
      "[PREDICTIONS] BATCH: 48/178\n",
      "[PREDICTIONS] BATCH: 49/178\n",
      "[PREDICTIONS] BATCH: 50/178\n",
      "[PREDICTIONS] BATCH: 51/178\n",
      "[PREDICTIONS] BATCH: 52/178\n",
      "[PREDICTIONS] BATCH: 53/178\n",
      "[PREDICTIONS] BATCH: 54/178\n",
      "[PREDICTIONS] BATCH: 55/178\n",
      "[PREDICTIONS] BATCH: 56/178\n",
      "[PREDICTIONS] BATCH: 57/178\n",
      "[PREDICTIONS] BATCH: 58/178\n",
      "[PREDICTIONS] BATCH: 59/178\n",
      "[PREDICTIONS] BATCH: 60/178\n",
      "[PREDICTIONS] BATCH: 61/178\n",
      "[PREDICTIONS] BATCH: 62/178\n",
      "[PREDICTIONS] BATCH: 63/178\n",
      "[PREDICTIONS] BATCH: 64/178\n",
      "[PREDICTIONS] BATCH: 65/178\n",
      "[PREDICTIONS] BATCH: 66/178\n",
      "[PREDICTIONS] BATCH: 67/178\n",
      "[PREDICTIONS] BATCH: 68/178\n",
      "[PREDICTIONS] BATCH: 69/178\n",
      "[PREDICTIONS] BATCH: 70/178\n",
      "[PREDICTIONS] BATCH: 71/178\n",
      "[PREDICTIONS] BATCH: 72/178\n",
      "[PREDICTIONS] BATCH: 73/178\n",
      "[PREDICTIONS] BATCH: 74/178\n",
      "[PREDICTIONS] BATCH: 75/178\n",
      "[PREDICTIONS] BATCH: 76/178\n",
      "[PREDICTIONS] BATCH: 77/178\n",
      "[PREDICTIONS] BATCH: 78/178\n",
      "[PREDICTIONS] BATCH: 79/178\n",
      "[PREDICTIONS] BATCH: 80/178\n",
      "[PREDICTIONS] BATCH: 81/178\n",
      "[PREDICTIONS] BATCH: 82/178\n",
      "[PREDICTIONS] BATCH: 83/178\n",
      "[PREDICTIONS] BATCH: 84/178\n",
      "[PREDICTIONS] BATCH: 85/178\n",
      "[PREDICTIONS] BATCH: 86/178\n",
      "[PREDICTIONS] BATCH: 87/178\n",
      "[PREDICTIONS] BATCH: 88/178\n",
      "[PREDICTIONS] BATCH: 89/178\n",
      "[PREDICTIONS] BATCH: 90/178\n",
      "[PREDICTIONS] BATCH: 91/178\n",
      "[PREDICTIONS] BATCH: 92/178\n",
      "[PREDICTIONS] BATCH: 93/178\n",
      "[PREDICTIONS] BATCH: 94/178\n",
      "[PREDICTIONS] BATCH: 95/178\n",
      "[PREDICTIONS] BATCH: 96/178\n",
      "[PREDICTIONS] BATCH: 97/178\n",
      "[PREDICTIONS] BATCH: 98/178\n",
      "[PREDICTIONS] BATCH: 99/178\n",
      "[PREDICTIONS] BATCH: 100/178\n",
      "[PREDICTIONS] BATCH: 101/178\n",
      "[PREDICTIONS] BATCH: 102/178\n",
      "[PREDICTIONS] BATCH: 103/178\n",
      "[PREDICTIONS] BATCH: 104/178\n",
      "[PREDICTIONS] BATCH: 105/178\n",
      "[PREDICTIONS] BATCH: 106/178\n",
      "[PREDICTIONS] BATCH: 107/178\n",
      "[PREDICTIONS] BATCH: 108/178\n",
      "[PREDICTIONS] BATCH: 109/178\n",
      "[PREDICTIONS] BATCH: 110/178\n",
      "[PREDICTIONS] BATCH: 111/178\n",
      "[PREDICTIONS] BATCH: 112/178\n",
      "[PREDICTIONS] BATCH: 113/178\n",
      "[PREDICTIONS] BATCH: 114/178\n",
      "[PREDICTIONS] BATCH: 115/178\n",
      "[PREDICTIONS] BATCH: 116/178\n",
      "[PREDICTIONS] BATCH: 117/178\n",
      "[PREDICTIONS] BATCH: 118/178\n",
      "[PREDICTIONS] BATCH: 119/178\n",
      "[PREDICTIONS] BATCH: 120/178\n",
      "[PREDICTIONS] BATCH: 121/178\n",
      "[PREDICTIONS] BATCH: 122/178\n",
      "[PREDICTIONS] BATCH: 123/178\n",
      "[PREDICTIONS] BATCH: 124/178\n",
      "[PREDICTIONS] BATCH: 125/178\n",
      "[PREDICTIONS] BATCH: 126/178\n",
      "[PREDICTIONS] BATCH: 127/178\n",
      "[PREDICTIONS] BATCH: 128/178\n",
      "[PREDICTIONS] BATCH: 129/178\n",
      "[PREDICTIONS] BATCH: 130/178\n",
      "[PREDICTIONS] BATCH: 131/178\n",
      "[PREDICTIONS] BATCH: 132/178\n",
      "[PREDICTIONS] BATCH: 133/178\n",
      "[PREDICTIONS] BATCH: 134/178\n",
      "[PREDICTIONS] BATCH: 135/178\n",
      "[PREDICTIONS] BATCH: 136/178\n",
      "[PREDICTIONS] BATCH: 137/178\n",
      "[PREDICTIONS] BATCH: 138/178\n",
      "[PREDICTIONS] BATCH: 139/178\n",
      "[PREDICTIONS] BATCH: 140/178\n",
      "[PREDICTIONS] BATCH: 141/178\n",
      "[PREDICTIONS] BATCH: 142/178\n",
      "[PREDICTIONS] BATCH: 143/178\n",
      "[PREDICTIONS] BATCH: 144/178\n",
      "[PREDICTIONS] BATCH: 145/178\n",
      "[PREDICTIONS] BATCH: 146/178\n",
      "[PREDICTIONS] BATCH: 147/178\n",
      "[PREDICTIONS] BATCH: 148/178\n",
      "[PREDICTIONS] BATCH: 149/178\n",
      "[PREDICTIONS] BATCH: 150/178\n",
      "[PREDICTIONS] BATCH: 151/178\n",
      "[PREDICTIONS] BATCH: 152/178\n",
      "[PREDICTIONS] BATCH: 153/178\n",
      "[PREDICTIONS] BATCH: 154/178\n",
      "[PREDICTIONS] BATCH: 155/178\n",
      "[PREDICTIONS] BATCH: 156/178\n",
      "[PREDICTIONS] BATCH: 157/178\n",
      "[PREDICTIONS] BATCH: 158/178\n",
      "[PREDICTIONS] BATCH: 159/178\n",
      "[PREDICTIONS] BATCH: 160/178\n",
      "[PREDICTIONS] BATCH: 161/178\n",
      "[PREDICTIONS] BATCH: 162/178\n",
      "[PREDICTIONS] BATCH: 163/178\n",
      "[PREDICTIONS] BATCH: 164/178\n",
      "[PREDICTIONS] BATCH: 165/178\n",
      "[PREDICTIONS] BATCH: 166/178\n",
      "[PREDICTIONS] BATCH: 167/178\n",
      "[PREDICTIONS] BATCH: 168/178\n",
      "[PREDICTIONS] BATCH: 169/178\n",
      "[PREDICTIONS] BATCH: 170/178\n",
      "[PREDICTIONS] BATCH: 171/178\n",
      "[PREDICTIONS] BATCH: 172/178\n",
      "[PREDICTIONS] BATCH: 173/178\n",
      "[PREDICTIONS] BATCH: 174/178\n",
      "[PREDICTIONS] BATCH: 175/178\n",
      "[PREDICTIONS] BATCH: 176/178\n",
      "[PREDICTIONS] BATCH: 177/178\n",
      "[PREDICTIONS] BATCH: 178/178\n",
      "SPAM: 0.9985\n",
      "TOXIC: 0.9332\n",
      "OK: 0.9493\n",
      "TOTAL: 0.9570\n"
     ]
    }
   ],
   "source": [
    "accuracy_results = calculate_class_accuracy(model, dl_valid)\n",
    "for class_name, acc in accuracy_results.items():\n",
    "    print(f\"{class_name}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1765cb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fa905bf2224014819deb55df53e571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a80a97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"./analysis/neural_training/gpt2_spam_toxic_classifier/\")\n",
    "model.save_pretrained(\"./analysis/neural_training/gpt2_spam_toxic_classifier/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8016efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"arseso/gpt2_spam_toxic_classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25a2a960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c09e9d159e45df96c5492c1b4209a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/31.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\progaem\\python\\min_messenger\\src\\ContentFilter\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\arses\\.cache\\huggingface\\hub\\models--arseso--gpt2_spam_toxic_classifier. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f6176749cc4406aaba7150268625eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16370a96822a4b218f9edb8a65ee1bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/31.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/arseso/gpt2_spam_toxic_classifier/commit/a0b03ea0fd3952f2d103d99cea220c3458962f6f', commit_message='Upload tokenizer for ruGPT-3 small', commit_description='', oid='a0b03ea0fd3952f2d103d99cea220c3458962f6f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/arseso/gpt2_spam_toxic_classifier', endpoint='https://huggingface.co', repo_type='model', repo_id='arseso/gpt2_spam_toxic_classifier'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\n",
    "    repo_id,\n",
    "    commit_message=\"Upload trained ruGPT-3 small spam/toxic classification model\"\n",
    ")\n",
    "\n",
    "# Загружаем токенизатор\n",
    "tokenizer.push_to_hub(\n",
    "    repo_id,\n",
    "    commit_message=\"Upload tokenizer for ruGPT-3 small\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
